{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c09ebe-6363-4c11-9d58-49b9c3ceb6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados salvos no arquivo Excel: 'C:\\Users\\tiago_igx865i\\Downloads\\BOT\\scraping_detalhado.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# URL base do repositório\n",
    "base_url = \"https://github.com/petrobras/3W/stargazers\"\n",
    "\n",
    "# Lista para armazenar os dados\n",
    "data = []\n",
    "\n",
    "# Função para realizar o scraping\n",
    "def scrape_stargazers():\n",
    "    page = 1\n",
    "    while True:\n",
    "        # Adiciona o número da página à URL\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        # Faz a requisição\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Verifica se a página existe\n",
    "        if response.status_code != 200:\n",
    "            print(\"Fim das páginas ou erro ao acessar.\")\n",
    "            break\n",
    "\n",
    "        # Analisa o HTML com BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Localiza os elementos contendo os perfis\n",
    "        stargazers = soup.find_all(\"a\", class_=\"d-inline-block\")\n",
    "\n",
    "        # Se não houver perfis, termina o loop\n",
    "        if not stargazers:\n",
    "            break\n",
    "\n",
    "        # Extrai os dados dos perfis\n",
    "        for stargazer in stargazers:\n",
    "            # Extrai o nome do perfil e o link\n",
    "            name = stargazer.find(\"img\")[\"alt\"].replace(\"@\", \"\").strip()  # Nome correto\n",
    "            profile_link = f\"https://github.com{stargazer['href']}\"          # Link do perfil\n",
    "\n",
    "            # Faz uma requisição para a página do perfil\n",
    "            profile_response = requests.get(profile_link)\n",
    "            if profile_response.status_code == 200:\n",
    "                profile_soup = BeautifulSoup(profile_response.text, \"html.parser\")\n",
    "\n",
    "                # Extrai informações adicionais do perfil ou define como vazio\n",
    "                name_tag = profile_soup.find(\"span\", class_=\"p-name\")\n",
    "                full_name = name_tag.text.strip() if name_tag else \"\"\n",
    "\n",
    "                location_tag = profile_soup.find(\"span\", class_=\"p-label\")\n",
    "                location = location_tag.text.strip() if location_tag else \"\"\n",
    "\n",
    "                company_tag = profile_soup.find(\"span\", class_=\"p-org\")\n",
    "                company = company_tag.text.strip() if company_tag else \"\"\n",
    "\n",
    "                utc_tag = profile_soup.find(\"relative-time\")\n",
    "                utc = utc_tag[\"datetime\"] if utc_tag else \"\"\n",
    "\n",
    "                # Capturando os links corretamente\n",
    "                links = [a['href'] for a in profile_soup.find_all('a', {'class': 'Link--primary'}) if 'href' in a.attrs]\n",
    "\n",
    "                # Adicionando os dados coletados à lista\n",
    "                dados = {\n",
    "                    \"GitHub\": name,\n",
    "                    \"URL\": profile_link,\n",
    "                    \"Nome\": full_name,\n",
    "                    \"Localização\": location,\n",
    "                    \"Empresa\": company,\n",
    "                    \"UTC\": utc\n",
    "                }\n",
    "\n",
    "                # Adicionando os links em colunas separadas\n",
    "                for i, link in enumerate(links):\n",
    "                    dados[f\"Link {i+1}\"] = link\n",
    "\n",
    "                data.append(dados)\n",
    "\n",
    "            # Aguarda 1 segundo antes de acessar o próximo perfil\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Avança para a próxima página\n",
    "        page += 1\n",
    "\n",
    "# Executa o scraping\n",
    "scrape_stargazers()\n",
    "\n",
    "# Converte os dados para um DataFrame e salva em um arquivo Excel\n",
    "output_path = r\"C:\\Users\\tiago_igx865i\\Downloads\\BOT\\scraping.xlsx\"\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Dados salvos no arquivo Excel: '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7aa4c2-1112-407b-9e96-f1622745ddff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
